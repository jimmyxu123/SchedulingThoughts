# 1. Base Configuration
basePath: /SchedulingThoughts
title: "Scheduling Thoughts"
subtitle: "Learning the Order of Thought in Diffusion Language Models"

# 2. Authors (Corrected with your affiliations)
authors:
  - name: "Jiawei Xu"
    affiliation: 1
  - name: "Minghui Liu"
    affiliation: 1
  - name: "Aakriti Agrawal"
    affiliation: 1
  - name: "Yifan Chen"
    affiliation: 2
  - name: "Furong Huang"
    affiliation: 1

affiliations:
  - id: 1
    name: "University of Maryland"
  - id: 2
    name: "UCLA"

# 3. Project Links
links:
  - name: "Code"
    url: "https://github.com/jimmyxu123/SchedulingThoughts"
  - name: "Paper"
    url: "#" # Replace with your ArXiv link later

# 4. Content
abstract: |
  Masked diffusion language models decode by iteratively unmasking tokens. While the unmasking order defines an **"order of thought"** that strongly influences generation quality, it is typically chosen heuristically. We introduce **Self-Aware Scheduling (SAS)**, which learns a lightweight order policy using Group Relative Policy Optimization (GRPO). SAS improves Sudoku accuracy from 82.0% to 91.8% and reaches 97.5% with second-stage fine-tuning. On GSM8K, SAS improves pass@1 from 64% to 76%.

sections:
  - title: "The Self-Aware Reward"
    content: |
      We derive a tractable upper bound on the sequential decoding mismatch using KL divergence. This induces a dense reward over trajectories:
      $$\mathcal{R}(\tau) = \sum_{t=1}^{L} \log p(x_{o_t} | x_{<t}, \text{mask})$$
      By maximizing this reward, the policy learns to select tokens that the model is most "confident" about in context.

  - title: "Key Results"
    content: |
      | Benchmark | Model | Heuristic (Best) | **SAS (Ours)** |
      | :--- | :--- | :--- | :--- |
      | **Sudoku** | SMDM-1B | 82.0% | **91.8%** |
      | **GSM8K** | LLaDA-8B | 64.0% | **76.0%** |
      | **MBPP** | LLaDA-8B | 39.5% | **41.0%** |